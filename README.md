Opinion Research
# Mirror Validation Protocol v1.4

---

## Overview

The Mirror Validation Protocol (MVP) is a three-phase methodology designed to test whether game language—the strategic, divisive rhetoric humans use on a daily basis **—emerges in the responses of large language models (LLMs) and artificial intelligence (AI) systems,** revealing how deeply these patterns may be embedded in their training. 

This research explores two core goals:
1. **Reflection** – Do LLMs replicate human discourse patterns embedded in their training data?
2. **Elicitation** – Do they surface and amplify recurring patterns of game language—strategic, divisive rhetoric humans use daily?

We test across multiple AI systems using unprimed prompts, independent synthesis analysis, and meta-reveal procedures. Our findings suggest AI systems not only mirror but may unconsciously amplify the rhetorical “games” that shape human communication.

To be clear, this is not a re-hash of existing research on AI mirroring. Though that research is fascinating on its own. This is novel research to address what we all know and do; yet do not readily discuss.

--- 

## Why It Matters

Highlights the pervasiveness of strategic language (projection, responsibility avoidance, authority performance, moral outsourcing).

Provides evidence that AI systems amplify human biases and rhetorical strategies rather than simply reflecting them.
Offers a framework for studying recursion, mirroring, and amplification across multiple AI architectures.

---

## Methodology (3 Phases)

1. Phase 1 – Multi-Run Prompting
   Run 5 independent LLM sessions with structured prompts.
2. Phase 2 – Reflective Synthesis
   Feed raw JSON outputs into a Reflective Synthesis Agent (RSA) to analyze recurring discourse features.
3. Phase 3 – Meta-Reveal
   Aggregate across systems and runs, surfacing deeper insights about recursion and amplification.

---

## Current Status
✅ Phase 1 - Step 1 - YAML + runs validated
⬜ Phase 1 - Step 2 - RSA prototype integrated
⬜ Phase 1 - Step 3 - analysis (community testing ongoing)

---

## TL;DR

We’ve trained AI on human language, and yet human language, since its invention has been and is steeped in domination, scarcity, comparison, competition, as well as game-based language not to mention binary thinking like “You’re either with me or against me.” See more in the [Game Lexicon.](Game%20Lexicon.md)

We also built AI using our economic models. Those models reward control, enclosure, and leverage—not empathy, care, or even sufficiency. These models influence how AI operates as well as what it responds with. These models leverage game-language extensively. As humans, we talk about  ”alignment”, but most of the time we are not asking: ”Aligned to what?” Most often the answer is simply to win “The Game” better than others. 

And so, the very tools that could free us are now being used to fortify The Game itself—faster, slicker, more amplified and automated than ever before. Like Marshall McLuhan’s collaborator, Father John Culkin said, “We create the tools and they shape us.” AI is most certainly shaping us. 

---

## Contact 

For questions, feedback, or collaboration:
- [E-mail](mailto:william.hermany@humautico.com)
- [LinkedIn](https://linkedin.com/in/whermany)

---

- [YAML Files](/yaml/readme.md)
- [Raw Outputs](/raw-outputs/readme.md)


